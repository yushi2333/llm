{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer的pythoch实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "使用transformer执行文本翻译任务，数据集选用英文和法语数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading spacy tokenizers...\n",
      "creating dataset and iterator... \n",
      "1090\n",
      "第一个batch的源语言（索引）:\n",
      "tensor([[  13,   15,   73,  ...,    3,   13,   13],\n",
      "        [  65,   67,   26,  ...,   20,   16,   51],\n",
      "        [  88,    8,    4,  ...,    5,  298,  801],\n",
      "        ...,\n",
      "        [  94,    5,   79,  ...,    8,  293,   85],\n",
      "        [  47, 3519, 2640,  ...,  653,  549,  607],\n",
      "        [   2,    2,    7,  ...,    2,    2,    2]])\n",
      "第一个batch的目标语言（索引）:\n",
      "tensor([[    2,     2,     2,  ...,     2,     2,     2],\n",
      "        [   11,    24,    83,  ...,     5,    36,    11],\n",
      "        [   33,    18,    57,  ...,   145,    10,  1309],\n",
      "        ...,\n",
      "        [  163,  4650, 13618,  ...,   658,   354,   189],\n",
      "        [    4,     4,     7,  ...,     4,  1219,     4],\n",
      "        [    3,     3,     3,  ...,     3,     3,     3]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from batch import create_masks\n",
    "from process import *\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 数据\n",
    "src_file = 'data/english.txt'\n",
    "trg_file = 'data/french.txt'\n",
    "src_lang = 'en_core_web_sm'\n",
    "trg_lang = 'fr_core_news_sm'\n",
    "max_strlen = 80\n",
    "batchsize = 1500\n",
    "src_data, trg_data = read_data(src_file, trg_file)  # 一个包含所有源语言（英语）句子的字符串列表。154883\n",
    "EN_TEXT, FR_TEXT = create_fields(src_lang, trg_lang)\n",
    "train_iter, src_pad, trg_pad = create_dataset(src_data, trg_data, EN_TEXT, FR_TEXT, max_strlen, batchsize) # , 1 , 1\n",
    "src_vocab = len(EN_TEXT.vocab) #13724 源语句的token数量\n",
    "trg_vocab = len(FR_TEXT.vocab) #23469 目标语句的token数量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个batch的源语言（索引）:\n",
      "tensor([[  41,  191,   27,  ...,   10,   25,   11],\n",
      "        [  36,   26,  322,  ...,    9,   30,   14],\n",
      "        [  44,  132,  266,  ...,   33,   24,   29],\n",
      "        [2497,  229, 5832,  ...,  650,  466,  372],\n",
      "        [   2,    2,    2,  ...,    2,    7,    7]])\n",
      "第一个batch的目标语言（索引）:\n",
      "torch.Size([7, 214])\n",
      "第一个batch第一句源语言（单词）:\n",
      "['they', \"'re\", 'all', 'terrified', '.']\n",
      "第一个batch第一句目标语言（单词）:\n",
      "['<sos>', 'elles', 'sont', 'toutes', 'terrifiées', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "# 获取训练集中的前2个batch，便于测试\n",
    "sample_batches = []\n",
    "for i, batch in enumerate(train_iter):\n",
    "    sample_batches.append(batch)\n",
    "    if i >= 0:  # 只取前1个batch\n",
    "        break\n",
    "\n",
    "# 打印第一个batch的源语言和目标语言内容（索引形式）\n",
    "print(\"第一个batch的源语言（索引）:\")\n",
    "print(sample_batches[0].src)\n",
    "print(\"第一个batch的目标语言（索引）:\")\n",
    "print(sample_batches[0].trg)\n",
    "# shape为torch.Size([7, 214])，214句话，每句话有7个token\n",
    "# 如果想要将索引还原为单词，可以这样做：\n",
    "src_vocab = EN_TEXT.vocab\n",
    "trg_vocab = FR_TEXT.vocab\n",
    "\n",
    "def indices_to_words(indices, vocab):\n",
    "    return [vocab.itos[idx] for idx in indices]\n",
    "\n",
    "# 以第一个batch的第一句话为例，转换为单词\n",
    "src_indices = sample_batches[0].src[:, 0]  # 第一句\n",
    "trg_indices = sample_batches[0].trg[:, 0]  # 第一句\n",
    "\n",
    "print(\"第一个batch第一句源语言（单词）:\")\n",
    "print(indices_to_words(src_indices, src_vocab))\n",
    "print(\"第一个batch第一句目标语言（单词）:\")\n",
    "print(indices_to_words(trg_indices, trg_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "heads = 8\n",
    "N = 6\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "![positional encoding](./picture/image1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.autograd import Variable\n",
    "class positionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=80, dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #根据输入语句的token数和嵌入向量的维度构造位置编码矩阵\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "                if i + 1 < d_model:\n",
    "                    pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 使得单词嵌入表示相对大一些\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        # 增加位置常量到单词嵌入表示中\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:, :seq_len, :], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
