{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer的pythoch实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "使用transformer执行文本翻译任务，数据集选用英文和法语数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading spacy tokenizers...\n",
      "creating dataset and iterator... \n",
      "1089\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from batch import *\n",
    "from process import *\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 数据\n",
    "src_file = 'data/english.txt'\n",
    "trg_file = 'data/french.txt'\n",
    "src_lang = 'en_core_web_sm'\n",
    "trg_lang = 'fr_core_news_sm'\n",
    "max_strlen = 80\n",
    "batchsize = 1500\n",
    "src_data, trg_data = read_data(src_file, trg_file)  # 一个包含所有源语言（英语）句子的字符串列表。154883\n",
    "EN_TEXT, FR_TEXT = create_fields(src_lang, trg_lang)\n",
    "train_iter, src_pad, trg_pad = create_dataset(src_data, trg_data, EN_TEXT, FR_TEXT, max_strlen, batchsize) # , 1 , 1\n",
    "src_vocab = len(EN_TEXT.vocab) #13724 源语句的token数量\n",
    "trg_vocab = len(FR_TEXT.vocab) #23469 目标语句的token数量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个batch的源语言（索引）:\n",
      "tensor([[  31,    3,    3,  ...,   13,   15,    3],\n",
      "        [ 290,   49,   10,  ..., 2190,   11,   10],\n",
      "        [  47,   12,   25,  ...,   14,  106,    9],\n",
      "        ...,\n",
      "        [  43,   95,   32,  ...,   23,  105,  106],\n",
      "        [ 113, 1601,  125,  ...,  309,   84,  602],\n",
      "        [   2,    2,    2,  ...,    2,    2,    2]])\n",
      "第一个batch的目标语言（索引）:\n",
      "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
      "        [  32,    5,    5,  ...,   11,   24,    5],\n",
      "        [  21,   84,  159,  ...,   18,   10,   14],\n",
      "        ...,\n",
      "        [ 129, 6919,   78,  ...,  321,  207,  674],\n",
      "        [   4,    4,    4,  ...,    4,    4,    4],\n",
      "        [   3,    3,    3,  ...,    3,    3,    3]])\n",
      "第一个batch第一句源语言（单词）:\n",
      "['she', 'married', 'him', 'for', 'his', 'money', '.']\n",
      "第一个batch第一句目标语言（单词）:\n",
      "['<sos>', 'elle', \"l'\", 'a', 'épousé', 'pour', 'son', 'argent', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "# 获取训练集中的前2个batch，便于测试\n",
    "sample_batches = []\n",
    "for i, batch in enumerate(train_iter):\n",
    "    sample_batches.append(batch)\n",
    "    if i >= 0:  # 只取前1个batch\n",
    "        break\n",
    "\n",
    "# 打印第一个batch的源语言和目标语言内容（索引形式）\n",
    "print(\"第一个batch的源语言（索引）:\")\n",
    "print(sample_batches[0].src)\n",
    "print(\"第一个batch的目标语言（索引）:\")\n",
    "print(sample_batches[0].trg)\n",
    "# shape为torch.Size([7, 214])，214句话，每句话有7个token\n",
    "# 如果想要将索引还原为单词，可以这样做：\n",
    "src_vocab_obj = EN_TEXT.vocab\n",
    "trg_vocab_obj = FR_TEXT.vocab\n",
    "\n",
    "def indices_to_words(indices, vocab):\n",
    "    return [vocab.itos[idx] for idx in indices]\n",
    "\n",
    "# 以第一个batch的第一句话为例，转换为单词\n",
    "src_indices = sample_batches[0].src[:, 0]  # 第一句\n",
    "trg_indices = sample_batches[0].trg[:, 0]  # 第一句\n",
    "\n",
    "print(\"第一个batch第一句源语言（单词）:\")\n",
    "print(indices_to_words(src_indices, src_vocab_obj))\n",
    "print(\"第一个batch第一句目标语言（单词）:\")\n",
    "print(indices_to_words(trg_indices, trg_vocab_obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "heads = 8\n",
    "N = 6\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "![positional encoding](./picture/image1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.autograd import Variable\n",
    "class positionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=80, dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #根据输入语句的token数和嵌入向量的维度构造位置编码矩阵\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "                if i + 1 < d_model:\n",
    "                    pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 使得单词嵌入表示相对大一些\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        # 增加位置常量到单词嵌入表示中\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:, :seq_len, :], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Layer\n",
    "对于输入的句子X，通过 WordEmbedding 得到该句子中每个字的字向量，同时通过Positional Encoding 得到所有字的位置向量，将其相加(维度相同，可以直接相加)，得到该字真正的向量表示。第t个字的向量记作$x_t$\n",
    "\n",
    "接着我们定义三个矩阵 $W_Q$,$W_K$,$W_V$,使用这三个矩阵分别对所有的字向量进行三次线性变换，于是所有的字向量又衍生出三个新的向量$q_t$,$k_t$,$v_t$。我们将所有的q向量拼成一个大矩阵，记作查询矩阵Q，将所有的k向量拼成一个大矩阵，记作键矩阵K，将所有的v向量拼成一个大矩阵，记作值矩阵V(见下图)\n",
    "\n",
    "![](./picture/image2.png)\n",
    "\n",
    "接下来将Q和$K^T$相乘，得到注意力分数矩阵，其中的每一行代表一个query(当前token)对所有的key的相关性分数，然后除以$\\sqrt{d_k}$(这是论文中提到的一个 trick),经过 softmax后,每一行是一个概率分布，表示该query“关注”每个key的程度，再乘以 V 得到输出，此时的每一行代表当前query（比如当前单词）在全局信息加权融合后的新表示，这个新表示综合了序列中所有位置的信息。\n",
    "\n",
    "![](./picture/image3.png)\n",
    "\n",
    "### Multi-Head Attention\n",
    "上面所定义的一组Q,K,V得到的融合向量只能从一个“视角”去捕捉信息，我们可以定义多组Q,K,V，让每个头关注不同的特则会给你、关系或位置，从而获得更丰富的信息表达。计算 Q,K,V 的过程还是一样，只不过线性变换的矩阵从一组($W^Q,W^K,W^V$)变成了多组$(W_0^Q,W_0^K,W_0^V),(W_1^Q,W_1^Q,W_1^Q)$,..如下图所示\n",
    "\n",
    "![](./picture/image4.png)\n",
    "\n",
    "对于输入矩阵 X，每一组 Q、K 和 V 都可以得到一个输出矩阵Z。如下图所示\n",
    "\n",
    "![](./picture/image5.png)\n",
    "\n",
    "### Src_input Paddding Mask\n",
    "对于encodee的源语句输入时，其中每个mini-batch是由多个不等长的句子组成的，我们需要按照这个mini-batch中最大的句长对剩余的句子进行补齐，一般用0进行填充，这个过程叫做 padding但这时在进行 softmax 就会产生问题。回顾softmax函数$\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$,$e^0$是1，是有值的，这样的话 softmax 中被 padding 的部分就参与了运算，相当于让无效的部分参与了运算，这可能会产生很大的隐患。因此需要做一个 mask 操作，让这些无效的区域不参与运算，一般是给无效区域加一个很大的负数偏置，即\n",
    "$$\n",
    "Z_{\\text{illegal}} = Z_{\\text{illegal}} + \\text{bias}_{\\text{illegal}}\n",
    "$$\n",
    "$$\n",
    "\\text{bias}_{\\text{illegal}} \\rightarrow -\\infty\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n",
    "        #(166*8*9*64) * (166*8*64*9) --> (166*8*9*9)  \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        # 此时mask的维度为(166*1*8)\n",
    "        # 掩盖那些为了补全长度而增加的单元，使其通过Softmax计算后为0\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "        \n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)\n",
    "\n",
    "        # 为了通过一个线性层实现多个头并行计算，将输入维度减小，即\n",
    "        # 输入向量的维度为(166*9*512)，经过线性层的维度仍为(166*9*512)，\n",
    "        # 接着改变特征维度为(166*9*8*64) (batch_size * max_length * heads * d_k)\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        # 矩阵转置\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        #multi_output的维度为（166*8*9*64）\n",
    "        multi_output = self.attention(q, k, v, self.d_k, mask, dropout=self.dropout)\n",
    "        \n",
    "        # 连接多个头并输入最后的线性层 （166*9*512）\n",
    "        concat = multi_output.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "\n",
    "        output = self.out(concat)\n",
    "\n",
    "        return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可重复的编码器层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.attn(x, x, x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, heads, N, vocab_size, max_length, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.N = N\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.dropout = dropout\n",
    "        self.embedding = Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = positionalEncoding(d_model, max_length)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
    "        #self.norm = Norm(d_model)\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embedding(src)\n",
    "        x = self.pos_encoding(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([166, 5, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##测试代码\n",
    "def create_mask(src, src_pad):\n",
    "    src_mask = (src != src_pad).unsqueeze(-2)\n",
    "    return src_mask\n",
    "\n",
    "encoder = Encoder(d_model, heads, N, src_vocab, max_strlen, dropout)\n",
    "for i, batch in enumerate(train_iter):\n",
    "    src = batch.src.transpose(0, 1)\n",
    "    src_mask = create_mask(src, src_pad)\n",
    "    pred = encoder(src, src_mask)\n",
    "    if(i == 1):\n",
    "        break\n",
    "pred.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
